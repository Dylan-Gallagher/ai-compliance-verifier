The notion of AI system in this Regulation should be clearly defined and closely aligned 
with the work of international organisations working on artificial intelligence to ensure 
legal certainty, facilitate international convergence and wide acceptance, while providing 
the flexibility to accommodate the rapid technological developments in this field. 
Moreover, it should be based on key characteristics of artificial intelligence systems, that 
distinguish it from simpler traditional software systems or programming approaches and 
should not cover systems that are based on the rules defined solely by natural persons to 
automatically execute operations. A key characteristic of AI systems is their capability to 
infer. This inference refers to the process of obtaining the outputs, such as predictions, 
content, recommendations, or decisions, which can influence physical and virtual 
environments and to a capability of AI systems to derive models and/or algorithms from 
inputs/data. The techniques that enable inference while building an AI system include 
machine learning approaches that learn from data how to achieve certain objectives; and 
logic- and knowledge-based approaches that infer from encoded knowledge or symbolic 
representation of the task to be solved. The capacity of an AI system to infer goes beyond 
basic data processing, enable learning, reasoning or modelling. The term “machine-based” 
refers to the fact that AI systems run on machines. The reference to explicit or implicit 
objectives underscores that AI systems can operate according to explicit defined objectives 
or to implicit objectives. The objectives of the AI system may be different from the 
intended purpose of the AI system in a specific context. For the purposes of this 
Regulation, environments should be understood as the contexts in which the AI systems 
operate, whereas outputs generated by the AI system, reflect different functions performed 
by AI systems and include predictions, content, recommendations or decisions. AI 
systems are designed to operate with varying levels of autonomy, meaning that they have 
some degree of independence of actions from human involvement and of capabilities to 
operate without human intervention. The adaptiveness that an AI system could exhibit after 
deployment, refers to self-learning capabilities, allowing the system to change while in use. 
AI systems can be used on a stand-alone basis or as a component of a product, irrespective 
of whether the system is physically integrated into the product (embedded) or serve the 
functionality of the product without being integrated therein (non-embedded).

While the risk-based approach is the basis for a proportionate and effective set of binding 
rules, it is important to recall the 2019 Ethics Guidelines for Trustworthy AI developed by 
the independent High-Level Expert Group on AI (HLEG) appointed by the Commission. 
In those Guidelines the HLEG developed seven non-binding ethical principles for AI 
which should help ensure that AI is trustworthy and ethically sound. The seven principles 
include: human agency and oversight; technical robustness and safety; privacy and data 
governance; transparency; diversity, non-discrimination and fairness; societal and 
environmental well-being and accountability. Without prejudice to the legally binding 
requirements of this Regulation and any other applicable Union law, these Guidelines 
contribute to the design of a coherent, trustworthy and human-centric Artificial 
Intelligence, in line with the Charter and with the values on which the Union is founded. 
According to the Guidelines of HLEG, human agency and oversight means that AI systems 
are developed and used as a tool that serves people, respects human dignity and personal 
autonomy, and that is functioning in a way that can be appropriately controlled and 
overseen by humans. Technical robustness and safety means that AI systems are developed 
and used in a way that allows robustness in case of problems and resilience against 
attempts to alter the use or performance of the AI system so as to allow unlawful use by 
third parties, and minimise unintended harm. Privacy and data governance means that AI 
systems are developed and used in compliance with existing privacy and data protection 
rules, while processing data that meets high standards in terms of quality and integrity. 
Transparency means that AI systems are developed and used in a way that allows 
appropriate traceability and explainability, while making humans aware that they 
communicate or interact with an AI system, as well as duly informing deployers of the 
capabilities and limitations of that AI system and affected persons about their rights. 
Diversity, non-discrimination and fairness means that AI systems are developed and used 
in a way that includes diverse actors and promotes equal access, gender equality and 
cultural diversity, while avoiding discriminatory impacts and unfair biases that are 
prohibited by Union or national law. Social and environmental well-being means that AI 
systems are developed and used in a sustainable and environmentally friendly manner as 
well as in a way to benefit all human beings, while monitoring and assessing the long-term 
impacts on the individual, society and democracy. The application of these principles 
should be translated, when possible, in the design and use of AI models. They should in 
any case serve as a basis for the drafting of codes of conduct under this Regulation. All 
stakeholders, including industry, academia, civil society and standardisation organisations, 
are encouraged to take into account as appropriate the ethical principles for the 
development of voluntary best practices and standards.

Biometric categorisation systems that are based on individuals’ biometric data, such as an 
individual person’s face or fingerprint, to deduce or infer an individuals’ political opinions, 
trade union membership, religious or philosophical beliefs, race, sex life or sexual 
orientation should be prohibited. This prohibition does not cover the lawful labelling, 
filtering or categorisation of biometric datasets acquired in line with Union or national law 
according to biometric data, such as the sorting of images according to hair colour or eye 
colour, which can for example be used in the area of law enforcement.

AI systems providing social scoring of natural persons by public or private actors may lead 
to discriminatory outcomes and the exclusion of certain groups. They may violate the right 
to dignity and non-discrimination and the values of equality and justice. Such AI systems 
evaluate or classify natural persons or groups thereof based on multiple data points related 
to their social behaviour in multiple contexts or known, inferred or predicted personal or 
personality characteristics over certain periods of time. The social score obtained from 
such AI systems may lead to the detrimental or unfavourable treatment of natural persons 
or whole groups thereof in social contexts, which are unrelated to the context in which the 
data was originally generated or collected or to a detrimental treatment that is
disproportionate or unjustified to the gravity of their social behaviour. AI systems entailing 
such unacceptable scoring practices leading to such detrimental or unfavourable outcomes 
should be therefore prohibited. This prohibition should not affect lawful evaluation 
practices of natural persons done for a specific purpose in compliance with national and 
Union law.

The use of those systems for the purpose of law enforcement should therefore be 
prohibited, except in exhaustively listed and narrowly defined situations, where the use is 
strictly necessary to achieve a substantial public interest, the importance of which 
outweighs the risks. Those situations involve the search for certain victims of crime 
including missing people; certain threats to the life or physical safety of natural persons or 
of a terrorist attack; and the localisation or identification of perpetrators or suspects of the 
criminal offences referred to in Annex IIa if those criminal offences are punishable in the 
Member State concerned by a custodial sentence or a detention order for a maximum 
period of at least four years and as they are defined in the law of that Member State. Such 
threshold for the custodial sentence or detention order in accordance with national law 
contributes to ensure that the offence should be serious enough to potentially justify the use 
of ‘real-time’ remote biometric identification systems. Moreover, the list of criminal 
offences as referred in Annex IIa is based on the 32 criminal offences listed in the Council 
Framework Decision 2002/584/JHA9
, taking into account that some are in practice likely 
to be more relevant than others, in that the recourse to ‘real-time’ remote biometric 
dentification will foreseeably be necessary and proportionate to highly varying degrees for 
the practical pursuit of the localisation or identification of a perpetrator or suspect of the 
different criminal offences listed and having regard to the likely differences in the 
seriousness, probability and scale of the harm or possible negative consequences. 
An imminent threat to life or physical safety of natural persons could also result from a 
serious disruption of critical infrastructure, as defined in Article 2, point (a) of Directive 
2008/114/EC, where the disruption or destruction of such critical infrastructure would 
result in an imminent threat to life or physical safety of a person, including through serious 
harm to the provision of basic supplies to the population or to the exercise of the core 
function of the State.
In addition, this Regulation should preserve the ability for law enforcement, border control, 
immigration or asylum authorities to carry out identity checks in the presence of the person 
that is concerned in accordance with the conditions set out in Union and national law for 
such checks. In particular, law enforcement, border control, immigration or asylum 
authorities should be able to use information systems, in accordance with Union or national 
law, to identify a person who, during an identity check, either refuses to be identified or is 
unable to state or prove his or her identity, without being required by this Regulation to 
obtain prior authorisation. This could be, for example, a person involved in a crime, being 
unwilling, or unable due to an accident or a medical condition, to disclose their identity to 
law enforcement authorities. 

The placing on the market, putting into service for this specific purpose, or use of AI 
systems that create or expand facial recognition databases through the untargeted scraping 
of facial images from the internet or CCTV footage should be prohibited, as this practice 
adds to the feeling of mass surveillance and can lead to gross violations of fundamental 
rights, including the right to privacy.

There are serious concerns about the scientific basis of AI systems aiming to identify or 
infer emotions, particularly as expression of emotions vary considerably across cultures 
and situations, and even within a single individual. Among the key shortcomings of such 
systems are the limited reliability, the lack of specificity and the limited generalizability. 
Therefore, AI systems identifying or inferring emotions or intentions of natural persons on 
the basis of their biometric data may lead to discriminatory outcomes and can be intrusive 
to the rights and freedoms of the concerned persons. Considering the imbalance of power 
in the context of work or education, combined with the intrusive nature of these systems, 
such systems could lead to detrimental or unfavourable treatment of certain natural persons 
or whole groups thereof. Therefore, the placing on the market, putting into service, or use 
of AI systems intended to be used to detect the emotional state of individuals in situations 
related to the workplace and education should be prohibited. This prohibition should not 
cover AI systems placed on the market strictly for medical or safety reasons, such as 
systems intended for therapeutical use. 

AI systems could have an adverse impact to health and safety of persons, in particular 
when such systems operate as safety components of products. Consistently with the 
objectives of Union harmonisation legislation to facilitate the free movement of products in 
the internal market and to ensure that only safe and otherwise compliant products find their 
way into the market, it is important that the safety risks that may be generated by a product 
as a whole due to its digital components, including AI systems, are duly prevented and 
mitigated. For instance, increasingly autonomous robots, whether in the context of 
manufacturing or personal assistance and care should be able to safely operate and 
performs their functions in complex environments. Similarly, in the health sector where the 
stakes for life and health are particularly high, increasingly sophisticated diagnostics 
systems and systems supporting human decisions should be reliable and accurate. 

It is also important to clarify that there may be specific cases in which AI systems referred 
to pre-defined areas specified in this Regulation do not lead to a significant risk of harm to 
the legal interests protected under those areas, because they do not materially influence the 
decision-making or do not harm those interests substantially. For the purpose of this 
Regulation an AI system not materially influencing the outcome of decision-making 
should be understood as an AI system that does not impact the substance, and thereby the 
outcome, of decision-making, whether human or automated. This could be the case if one 
or more of the following conditions are fulfilled. The first criterion should be that the AI 
system is intended to perform a narrow procedural task, such as an AI system that 
transforms unstructured data into structured data, an AI system that classifies incoming 
documents into categories or an AI system that is used to detect duplicates among a large 
number of applications. These tasks are of such narrow and limited nature that they pose 
only limited risks which are not increased through the use in a context listed in Annex III. 
The second criterion should be that the task performed by the AI system is intended to 
improve the result of a previously completed human activity that may be relevant for the
purpose of the use case listed in Annex III. Considering these characteristics, the AI system 
only provides an additional layer to a human activity with consequently lowered risk. For 
example, this criterion would apply to AI systems that are intended to improve the 
language used in previously drafted documents, for instance in relation to professional
tone, academic style of language or by aligning text to a certain brand messaging. The third 
criterion should be that the AI system is intended to detect decision-making patterns or 
deviations from prior decision-making patterns. The risk would be lowered because the use 
of the AI system follows a previously completed human assessment which it is not meant 
to replace or influence, without proper human review. Such AI systems include for 
instance those that, given a certain grading pattern of a teacher, can be used to check ex 
post whether the teacher may have deviated from the grading pattern so as to flag potential 
inconsistencies or anomalies. The fourth criterion should be that the AI system is intended 
to perform a task that is only preparatory to an assessment relevant for the purpose of the 
use case listed in Annex III, thus making the possible impact of the output of the system 
very low in terms of representing a risk for the assessment to follow. For example, this 
criterion covers smart solutions for file handling, which include various functions from 
indexing, searching, text and speech processing or linking data to other data sources, or AI 
systems used for translation of initial documents. In any case, AI systems referred to in 
Annex III should be considered to pose significant risks of harm to the health, safety or 
fundamental rights of natural persons if the AI system implies profiling within the meaning 
of Article 4(4) of Regulation (EU) 2016/679 and Article 3(4) of Directive (EU) 2016/680 
and Article 3(5) of Regulation 2018/1725. To ensure traceability and transparency, a 
provider who considers that an AI system referred to in Annex III is not high-risk on the 
basis of the aforementioned criteria should draw up documentation of the assessment 
before that system is placed on the market or put into service and should provide this 
documentation to national competent authorities upon request. Such provider should be 
obliged to register the system in the EU database established under this Regulation. With a 
view to provide further guidance for the practical implementation of the criteria under 
which AI systems referred to in Annex III are exceptionally not high-risk, the Commission 
should, after consulting the AI Board, provide guidelines specifying this practical 
implementation completed by a comprehensive list of practical examples of high risk and 
non-high risk use cases of AI systems.

As biometric data constitutes a special category of sensitive personal data, it is appropriate 
to classify as high-risk several critical use-cases of biometric systems, insofar as their use 
is permitted under relevant Union and national law. Technical inaccuracies of AI systems 
intended for the remote biometric identification of natural persons can lead to biased 
results and entail discriminatory effects. This is particularly relevant when it comes to age, 
ethnicity, race, sex or disabilities. Therefore, remote biometric identification systems
should be classified as high-risk in view of the risks that they pose. This excludes AI 
systems intended to be used for biometric verification, which includes authentication, 
whose sole purpose is to confirm that a specific natural person is the person he or she 
claims to be and to confirm the identity of a natural person for the sole purpose of having 
access to a service, unlocking a device or having secure access to premises.
In addition, AI systems intended to be used for biometric categorisation according to 
sensitive attributes or characteristics protected under Article 9(1) of Regulation (EU) 
2016/679 based on biometric data, in so far as these are not prohibited under this 
Regulation, and emotion recognition systems that are not prohibited under this Regulation, 
should be classified as high-risk. Biometric systems which are intended to be used solely 
for the purpose of enabling cybersecurity and personal data protection measures should not 
be considered as high-risk systems

As regards the management and operation of critical infrastructure, it is appropriate to 
classify as high-risk the AI systems intended to be used as safety components in the 
management and operation of critical digital infrastructure as listed in Annex I point 8 of 
the Directive on the resilience of critical entities, road traffic and the supply of water, gas, 
heating and electricity, since their failure or malfunctioning may put at risk the life and 
health of persons at large scale and lead to appreciable disruptions in the ordinary conduct 
of social and economic activities. Safety components of critical infrastructure, including 
critical digital infrastructure, are systems used to directly protect the physical integrity of 
critical infrastructure or health and safety of persons and property but which are not 
necessary in order for the system to function. Failure or malfunctioning of such 
components might directly lead to risks to the physical integrity of critical infrastructure 
and thus to risks to health and safety of persons and property. Components intended to be 
used solely for cybersecurity purposes should not qualify as safety components. Examples 
of safety components of such critical infrastructure may include systems for monitoring 
water pressure or fire alarm controlling systems in cloud computing centres.

Deployment of AI systems in education is important to promote high-quality digital 
education and training and to allow all learners and teachers to acquire and share the 
necessary digital skills and competences, including media literacy, and critical thinking, to 
take an active part in the economy, society, and in democratic processes. However, AI 
systems used in education or vocational training, notably for determining access or 
admission, for assigning persons to educational and vocational training institutions or
programmes at all levels, for evaluating learning outcomes of persons, for assessing the 
appropriate level of education for an individual and materially influencing the level of 
education and training that individuals will receive or be able to access or for monitoring 
and detecting prohibited behaviour of students during tests should be classified as high-risk 
AI systems, since they may determine the educational and professional course of a 
person’s life and therefore affect their ability to secure their livelihood. When improperly 
designed and used, such systems can be particularly intrusive and may violate the right to 
education and training as well as the right not to be discriminated against and perpetuate 
historical patterns of discrimination, for example against women, certain age groups, 
persons with disabilities, or persons of certain racial or ethnic origins or sexual orientation. 

 AI systems used in employment, workers management and access to self-employment, 
notably for the recruitment and selection of persons, for making decisions affecting terms 
of the work related relationship promotion and termination of work-related contractual 
relationships for allocating tasks based on individual behaviour, personal traits or 
characteristics and for monitoring or evaluation of persons in work-related contractual 
relationships, should also be classified as high-risk, since those systems may appreciably 
impact future career prospects, livelihoods of these persons and workers’ rights. Relevant 
work-related contractual relationships should meaningfully involve employees and persons 
providing services through platforms as referred to in the Commission Work Programme 
2021. Throughout the recruitment process and in the evaluation, promotion, or retention of 
persons in work-related contractual relationships, such systems may perpetuate historical 
patterns of discrimination, for example against women, certain age groups, persons with 
disabilities, or persons of certain racial or ethnic origins or sexual orientation. AI systems 
used to monitor the performance and behaviour of these persons may also undermine their 
fundamental rights to data protection and privacy. 

 Another area in which the use of AI systems deserves special consideration is the access to 
and enjoyment of certain essential private and public services and benefits necessary for 
people to fully participate in society or to improve one’s standard of living. In particular, 
natural persons applying for or receiving essential public assistance benefits and services 
from public authorities namely healthcare services, social security benefits, social services 
providing protection in cases such as maternity, illness, industrial accidents, dependency or 
old age and loss of employment and social and housing assistance, are typically dependent 
on those benefits and services and in a vulnerable position in relation to the responsible
authorities. If AI systems are used for determining whether such benefits and services 
should be granted, denied, reduced, revoked or reclaimed by authorities, including whether 
beneficiaries are legitimately entitled to such benefits or services, those systems may have 
a significant impact on persons’ livelihood and may infringe their fundamental rights, such 
as the right to social protection, non-discrimination, human dignity or an effective remedy 
and should therefore be classified as high-risk. Nonetheless, this Regulation should not 
hamper the development and use of innovative approaches in the public administration, 
which would stand to benefit from a wider use of compliant and safe AI systems, provided 
that those systems do not entail a high risk to legal and natural persons. In addition, AI
systems used to evaluate the credit score or creditworthiness of natural persons should be 
classified as high-risk AI systems, since they determine those persons’ access to financial 
resources or essential services such as housing, electricity, and telecommunication 
services. AI systems used for this purpose may lead to discrimination of persons or groups 
and perpetuate historical patterns of discrimination, for example based on racial or ethnic 
origins, gender, disabilities, age, sexual orientation, or create new forms of discriminatory 
impacts. However, AI systems provided for by Union law for the purpose of detecting 
fraud in the offering of financial services and for prudential purposes to calculate credit 
institutions’ and insurances undertakings’ capital requirements should not be considered as 
high-risk under this Regulation. Moreover, AI systems intended to be used for risk 
assessment and pricing in relation to natural persons for health and life insurance can also 
have a significant impact on persons’ livelihood and if not duly designed, developed and 
used, can infringe their fundamental rights and can lead to serious consequences for 
people’s life and health, including financial exclusion and discrimination. Finally, AI 
systems used to evaluate and classify emergency calls by natural persons or to dispatch or 
establish priority in the dispatching of emergency first response services, including by 
police, firefighters and medical aid, as well as of emergency healthcare patient triage 
systems, should also be classified as high-risk since they make decisions in very critical 
situations for the life and health of persons and their property.

Given their role and responsibility, actions by law enforcement authorities involving 
certain uses of AI systems are characterised by a significant degree of power imbalance 
and may lead to surveillance, arrest or deprivation of a natural person’s liberty as well as 
other adverse impacts on fundamental rights guaranteed in the Charter. In particular, if the 
AI system is not trained with high quality data, does not meet adequate requirements in 
terms of its performance, its accuracy or robustness, or is not properly designed and tested
before being put on the market or otherwise put into service, it may single out people in a 
discriminatory or otherwise incorrect or unjust manner. Furthermore, the exercise of 
important procedural fundamental rights, such as the right to an effective remedy and to a 
fair trial as well as the right of defence and the presumption of innocence, could be 
hampered, in particular, where such AI systems are not sufficiently transparent, 
explainable and documented. It is therefore appropriate to classify as high-risk, insofar as 
their use is permitted under relevant Union and national law, a number of AI systems 
intended to be used in the law enforcement context where accuracy, reliability and 
transparency is particularly important to avoid adverse impacts, retain public trust and 
ensure accountability and effective redress. In view of the nature of the activities in 
question and the risks relating thereto, those high-risk AI systems should include in 
particular AI systems intended to be used by or on behalf of law enforcement authorities or 
by Union agencies, offices or bodies in support of law enforcement authorities for 
assessing the risk of a natural person to become a victim of criminal offences, as 
polygraphs and similar tools , for the evaluation of the reliability of evidence in in the 
course of investigation or prosecution of criminal offences, and, insofar not prohibited 
under this regulation, for assessing the risk of a natural person of offending or reoffending 
not solely based on profiling of natural persons nor based on assessing personality traits 
and characteristics or past criminal behaviour of natural persons or groups, for profiling in 
the course of detection, investigation or prosecution of criminal offences, . AI systems 
specifically intended to be used for administrative proceedings by tax and customs 
authorities as well as by financial intelligence units carrying out administrative tasks 
analysing information pursuant to Union anti-money laundering legislation should not be 
classified as high-risk AI systems used by law enforcement authorities for the purposes of 
prevention, detection, investigation and prosecution of criminal offences. The use of AI 
tools by law enforcement and authorities should not become a factor of inequality, or 
exclusion. The impact of the use of AI tools on the defence rights of suspects should not be 
ignored, notably the difficulty in obtaining meaningful information on the functioning of 
these systems and the consequent difficulty in challenging their results in court, in 
particular by individuals under investigation

AI systems used in migration, asylum and border control management affect people who 
are often in particularly vulnerable position and who are dependent on the outcome of the 
actions of the competent public authorities. The accuracy, non-discriminatory nature and 
transparency of the AI systems used in those contexts are therefore particularly important
to guarantee the respect of the fundamental rights of the affected persons, notably their 
rights to free movement, non-discrimination, protection of private life and personal data, 
international protection and good administration. It is therefore appropriate to classify as 
high-risk, insofar as their use is permitted under relevant Union and national law AI 
systems intended to be used by or on behalf of competent public authorities or by Union 
agencies, offices or bodies charged with tasks in the fields of migration, asylum and border 
control management as polygraphs and similar tools, for assessing certain risks posed by 
natural persons entering the territory of a Member State or applying for visa or asylum, for 
assisting competent public authorities for the examination, including related assessment of 
the reliability of evidence, of applications for asylum, visa and residence permits and 
associated complaints with regard to the objective to establish the eligibility of the natural 
persons applying for a status, for the purpose of detecting, recognising or identifying 
natural persons in the context of migration, asylum and border control management with 
the exception of travel documents. AI systems in the area of migration, asylum and border 
control management covered by this Regulation should comply with the relevant 
procedural requirements set by the Directive 2013/32/EU of the European Parliament and 
of the Council20, the Regulation (EC) No 810/2009 of the European Parliament and of the 
Council21 and other relevant legislation. The use of AI systems in migration, asylum and 
border control management should in no circumstances be used by Member States or 
Union institutions, agencies or bodies as a means to circumvent their international 
obligations under the Convention of 28 July 1951 relating to the Status of Refugees as 
amended by the Protocol of 31 January 1967, nor should they be used to in any way 
infringe on the principle of non-refoulement, or deny safe and effective legal avenues into 
the territory of the Union, including the right to international protection.

Certain AI systems intended for the administration of justice and democratic processes 
should be classified as high-risk, considering their potentially significant impact on 
democracy, rule of law, individual freedoms as well as the right to an effective remedy and 
to a fair trial. In particular, to address the risks of potential biases, errors and opacity, it is 
appropriate to qualify as high-risk AI systems intended to be used by a judicial authority or 
on its behalf to assist judicial authorities in researching and interpreting facts and the law 
and in applying the law to a concrete set of facts. AI systems intended to be used by
alternative dispute resolution bodies for those purposes should also be considered high-risk 
when the outcomes of the alternative dispute resolution proceedings produce legal effects 
for the parties. The use of artificial intelligence tools can support the decision-making 
power of judges or judicial independence, but should not replace it, as the final decision-
making must remain a human-driven activity and decision. Such qualification should not 
extend, however, to AI systems intended for purely ancillary administrative activities that 
do not affect the actual administration of justice in individual cases, such as anonymisation 
or pseudonymisation of judicial decisions, documents or data, communication between 
personnel, administrative tasks.

Without prejudice to the rules provided for in [Regulation xxx on the transparency and 
targeting of political advertising], and in order to address the risks of undue external 
interference to the right to vote enshrined in Article 39 of the Charter, and of adverse 
effects on democracy, and the rule of law, AI systems intended to be used to influence the 
outcome of an election or referendum or the voting behaviour of natural persons in the 
exercise of their vote in elections or referenda should be classified as high-risk AI systems 
with the exception of AI systems whose output natural persons are not directly exposed to, 
such as tools used to organise, optimise and structure political campaigns from an 
administrative and logistical point of view. 

The risk management system should consist of a continuous, iterative process that is 
planned and run throughout the entire lifecycle of a high-risk AI system. This process 
should be aimed at identifying and mitigating the relevant risks of artificial intelligence 
systems on health, safety and fundamental rights. The risk management system should be 
regularly reviewed and updated to ensure its continuing effectiveness, as well as 
justification and documentation of any significant decisions and actions taken subject to 
this Regulation. This process should ensure that the provider identifies risks or adverse 
impacts and implements mitigation measures for the known and reasonably foreseeable 
risks of artificial intelligence systems to the health, safety and fundamental rights in light 
of its intended purpose and reasonably foreseeable misuse, including the possible risks 
arising from the interaction between the AI system and the environment within which it 
operates. The risk management system should adopt the most appropriate risk management 
measures in the light of the state of the art in AI. When identifying the most appropriate 
risk management measures, the provider should document and explain the choices made 
and, when relevant, involve experts and external stakeholders. In identifying reasonably 
foreseeable misuse of high-risk AI systems the provider should cover uses of the AI 
systems which, while not directly covered by the intended purpose and provided for in the 
instruction for use may nevertheless be reasonably expected to result from readily 
predictable human behaviour in the context of the specific characteristics and use of the 
particular AI system. Any known or foreseeable circumstances, related to the use of the 
high-risk AI system in accordance with its intended purpose or under conditions of 
reasonably foreseeable misuse, which may lead to risks to the health and safety or 
fundamental rights should be included in the instructions for use provided by the provider. 
This is to ensure that the deployer is aware and takes them into account when using the 
high-risk AI system. Identifying and implementing risk mitigation measures for 
foreseeable misuse under this Regulation should not require specific additional training 
measures for the high-risk AI system by the provider to address them. The providers 
however are encouraged to consider such additional training measures to mitigate 
reasonable foreseeable misuses as necessary and appropriate. 

High quality data and access to high quality data plays a vital role in providing structure 
and in ensuring the performance of many AI systems, especially when techniques 
involving the training of models are used, with a view to ensure that the high-risk AI 
system performs as intended and safely and it does not become a source of discrimination 
prohibited by Union law. High quality datasets for training, validation and testing require 
the implementation of appropriate data governance and management practices. Datasets for 
training, validation and testing, including the labels, should be relevant, sufficiently 
representative, and to the best extent possible free of errors and complete in view of the 
intended purpose of the system. In order to facilitate compliance with EU data protection 
law, such as Regulation (EU) 2016/679, data governance and management practices 
should include, in the case of personal data, transparency about the original purpose of the 
data collection, The datasets should also have the appropriate statistical properties, 
including as regards the persons or groups of persons in relation to whom the high-risk AI 
system is intended to be used, with specific attention to the mitigation of possible biases in 
the datasets, that are likely to affect the health and safety of persons, negatively impact 
fundamental rights or lead to discrimination prohibited under Union law, especially where 
data outputs influence inputs for future operations (‘feedback loops’) . Biases can for 
example be inherent in underlying datasets, especially when historical data is being used, 
or generated when the systems are implemented in real world settings. Results provided by 
AI systems could be influenced by such inherent biases that are inclined to gradually 
increase and thereby perpetuate and amplify existing discrimination, in particular for 
persons belonging to certain vulnerable groups including racial or ethnic groups. The 
requirement for the datasets to be to the best extent possible complete and free of errors 
should not affect the use of privacy-preserving techniques in the context of the 
development and testing of AI systems. In particular, datasets should take into account, to 
the extent required by their intended purpose, the features, characteristics or elements that 
are particular to the specific geographical, contextual, behavioural or functional setting 
which the AI system is intended to be used. The requirements related to data governance 
can be complied with by having recourse to third parties that offer certified compliance 
services including verification of data governance, data set integrity, and data training, 
validation and testing practices, as far as compliance with the data requirements of this 
Regulation are ensured. 

The right to privacy and to protection of personal data must be guaranteed throughout the 
entire lifecycle of the AI system. In this regard, the principles of data minimisation and 
data protection by design and by default, as set out in Union data protection law, are 
applicable when personal data are processed. Measures taken by providers to ensure 
compliance with those principles may include not only anonymisation and encryption, but 
also the use of technology that permits algorithms to be brought to the data and allows 
training of AI systems without the transmission between parties or copying of the raw or 
structured data themselves, without prejudice to the requirements on data governance 
provided for in this Regulation.

Having comprehensible information on how high-risk AI systems have been developed 
and how they perform throughout their lifetime is essential to enable traceability of those 
systems, verify compliance with the requirements under this Regulation, as well as 
monitoring of their operations and post market monitoring. This requires keeping records 
and the availability of a technical documentation, containing information which is 
necessary to assess the compliance of the AI system with the relevant requirements and
facilitate post market monitoring. Such information should include the general 
characteristics, capabilities and limitations of the system, algorithms, data, training, testing 
and validation processes used as well as documentation on the relevant risk management 
system and drawn in a clear and comprehensive form. The technical documentation should 
be kept up to date, appropriately throughout the lifetime of the AI system. Furthermore, 
high risk AI systems should technically allow for automatic recording of events (logs) over 
the duration of the lifetime of the system.

To address concerns related to opacity and complexity of certain AI systems and help 
deployers to fulfil their obligations under this Regulation, transparency should be required 
for high-risk AI systems before they are placed on the market or put it into service. Highrisk AI systems should be designed in a manner to enable deployers to understand how the 
AI system works, evaluate its functionality, and comprehend its strengths and limitations. 
High-risk AI systems should be accompanied by appropriate information in the form of 
instructions of use. Such information should include the characteristics, capabilities and 
limitations of performance of the AI system. These would cover information on possible 
known and foreseeable circumstances related to the use of the high-risk AI system, 
including deployer action that may influence system behaviour and performance, under 
which the AI system can lead to risks to health, safety, and fundamental rights, on the 
changes that have been pre-determined and assessed for conformity by the provider and on 
the relevant human oversight measures, including the measures to facilitate the 
interpretation of the outputs of the AI system by the deployers. Transparency, including the 
accompanying instructions for use, should assist deployers in the use of the system and 
support informed decision making by them. Among others, deployers should be in a better 
position to make the correct choice of the system they intend to use in the light of the 
obligations applicable to them, be educated about the intended and precluded uses, and use 
the AI system correctly and as appropriate. In order to enhance legibility and accessibility 
of the information included in the instructions of use, where appropriate, illustrative 
examples, for instance on the limitations and on the intended and precluded uses of the AI 
system, should be included. Providers should ensure that all documentation, including the 
instructions for use, contains meaningful, comprehensive, accessible and understandable 
information, taking into account the needs and foreseeable knowledge of the target 
deployers. Instructions for use should be made available in a language which can be easily 
understood by target deployers, as determined by the Member State concerned. 

 High-risk AI systems should be designed and developed in such a way that natural persons 
can oversee their functioning, ensure that they are used as intended and that their impacts 
are addressed over the system’s lifecycle. For this purpose, appropriate human oversight 
measures should be identified by the provider of the system before its placing on the 
market or putting into service. In particular, where appropriate, such measures should 
guarantee that the system is subject to in-built operational constraints that cannot be 
overridden by the system itself and is responsive to the human operator, and that the 
natural persons to whom human oversight has been assigned have the necessary 
competence, training and authority to carry out that role. It is also essential, as appropriate, 
to ensure that high-risk AI systems include mechanisms to guide and inform a natural 
person to whom human oversight has been assigned to make informed decisions if, when 
and how to intervene in order to avoid negative consequences or risks, or stop the system if 
it does not perform as intended. Considering the significant consequences for persons in 
case of incorrect matches by certain biometric identification systems, it is appropriate to 
provide for an enhanced human oversight requirement for those systems so that no action 
or decision may be taken by the deployer on the basis of the identification resulting from 
the system unless this has been separately verified and confirmed by at least two natural 
persons. Those persons could be from one or more entities and include the person 
operating or using the system. This requirement should not pose unnecessary burden or 
delays and it could be sufficient that the separate verifications by the different persons are 
automatically recorded in the logs generated by the system. Given the specificities of the 
areas of law enforcement, migration, border control and asylum, this requirement should 
not apply in cases where Union or national law considers the application of this 
requirement to be disproportionate. 

 The technical robustness is a key requirement for high-risk AI systems. They should be 
resilient in relation to harmful or otherwise undesirable behaviour that may result from 
limitations within the systems or the environment in which the systems operate (e.g. errors, 
faults, inconsistencies, unexpected situations). Therefore, technical and organisational 
measures should be taken to ensure robustness of high-risk AI systems, for example by 
designing and developing appropriate technical solutions to prevent or minimize harmful 
or otherwise undesirable behaviour. Those technical solution may include for instance 
mechanisms enabling the system to safely interrupt its operation (fail-safe plans) in the 
presence of certain anomalies or when operation takes place outside certain predetermined 
boundaries. Failure to protect against these risks could lead to safety impacts or negatively 
affect the fundamental rights, for example due to erroneous decisions or wrong or biased 
outputs generated by the AI system.

 General purpose AI systems may be used as high-risk AI systems by themselves or be 
components of other high risk AI systems. Therefore, due to their particular nature and in 
order to ensure a fair sharing of responsibilities along the AI value chain, the providers of 
such systems should, irrespective of whether they may be used as high-risk AI systems as 
such by other providers or as components of high-risk AI systems and unless provided 
otherwise under this Regulation, closely cooperate with the providers of the respective 
high-risk AI systems to enable their compliance with the relevant obligations under this 
Regulation and with the competent authorities established under this Regulation. 

Third parties making accessible to the public tools, services, processes, or AI components 
other than general-purpose AI models, shall not be mandated to comply with requirements 
targeting the responsibilities along the AI value chain, in particular towards the provider 
that has used or integrated them, when those tools, services, processes, or AI components 
are made accessible under a free and open licence. Developers of free and open-source 
tools, services, processes, or AI components other than general-purpose AI models should 
be encouraged to implement widely adopted documentation practices, such as model cards 
and data sheets, as a way to accelerate information sharing along the AI value chain, 
allowing the promotion of trustworthy AI systems in the Union. 

Whilst risks related to AI systems can result from the way such systems are designed, risks 
can as well stem from how such AI systems are used. Deployers of high-risk AI system 
therefore play a critical role in ensuring that fundamental rights are protected, 
complementing the obligations of the provider when developing the AI system. Deployers 
are best placed to understand how the high-risk AI system will be used concretely and can 
therefore identify potential significant risks that were not foreseen in the development 
phase, due to a more precise knowledge of the context of use, the people or groups of 
people likely to be affected, including vulnerable groups. Deployers of high-risk AI 
systems referred to in Annex III also play a critical role in informing natural persons and 
should, when they make decisions or assist in making decisions related to natural persons, 
where applicable, inform the natural persons that they are subject to the use of the high risk 
AI system. This information should include the intended purpose and the type of decisions 
it makes. The deployer should also inform the natural person about its right to an 
explanation provided under this Regulation. With regard to high-risk AI systems used for
law enforcement purposes, this obligation should be implemented in accordance with 
Article 13 of Directive 2016/680. 

In order to efficiently ensure that fundamental rights are protected, deployers of high-risk 
AI systems that are bodies governed by public law, or private operators providing public 
services and operators deploying certain high-risk AI system referred to in Annex III, such 
as banking or insurance entities, should carry out a fundamental rights impact assessment 
prior to putting it into use. Services important for individuals that are of public nature may 
also be provided by private entities. Private operators providing such services of public
nature are linked to tasks in the public interest such as in the area of education, healthcare, 
social services, housing, administration of justice. The aim of the fundamental rights 
impact assessment is for the deployer to identify the specific risks to the rights of 
individuals or groups of individuals likely to be affected, identify measures to be taken in 
case of the materialisation of these risks. The impact assessment should apply to the first  
use of the high-risk AI system, and should be updated when the deployer considers that 
any of the relevant factors have changed. The impact assessment should identify the 
deployer’s relevant processes in which the high-risk AI system will be used in line with its 
intended purpose, and should include a description of the period of time and frequency in 
which the system is intended to be used as well as of specific categories of natural persons 
and groups who are likely to be affected in the specific context of use. The assessment 
should also include the identification of specific risks of harm likely to impact the 
fundamental rights of these persons or groups. While performing this assessment, the 
deployer should take into account information relevant to a proper assessment of impact, 
including but not limited to the information given by the provider of the high-risk AI 
system in the instructions for use. In light of the risks identified, deployers should 
determine measures to be taken in case of the materialization of these risks, including for 
example governance arrangements in that specific context of use, such as arrangements for 
human oversight according to the instructions of use or, complaint handling and redress 
procedures, as they could be instrumental in mitigating risks to fundamental rights in 
concrete use-cases. After performing this impact assessment, the deployer should notify 
the relevant market surveillance authority. Where appropriate, to collect relevant 
information necessary to perform the impact assessment, deployers of high-risk AI system, 
in particular when AI systems are used in the public sector, could involve relevant 
stakeholders, including the representatives of groups of persons likely to be affected by the 
AI system, independent experts, and civil society organisations in conducting such impact 
assessments and designing measures to be taken in the case of materialization of the risks. 
The AI Office should develop a template for a questionnaire in order to facilitate 
compliance and reduce the administrative burden for deployers.

The notion of general purpose AI models should be clearly defined and set apart from the 
notion of AI systems to enable legal certainty. The definition should be based on the key 
functional characteristics of a general-purpose AI model, in particular the generality and 
the capability to competently perform a wide range of distinct tasks. These models are 
typically trained on large amounts of data, through various methods, such as selfsupervised, unsupervised or reinforcement learning. General purpose AI models may be 
placed on the market in various ways, including through libraries, application 
programming interfaces (APIs), as direct download, or as physical copy. These models 
may be further modified or fine-tuned into new models. Although AI models are essential 
components of AI systems, they do not constitute AI systems on their own. AI models
require the addition of further components, such as for example a user interface, to become 
AI systems. AI models are typically integrated into and form part of AI systems. This 
Regulation provides specific rules for general purpose AI models and for general purpose 
AI models that pose systemic risks, which should apply also when these models are 
integrated or form part of an AI system. It should be understood that the obligations for the 
providers of general purpose AI models should apply once the general purpose AI models 
are placed on the market. When the provider of a general purpose AI model integrates an 
own model into its own AI system that is made available on the market or put into service, 
that model should be considered as being placed on the market and, therefore, the 
obligations in this Regulation for models should continue to apply in addition to those for 
AI systems. The obligations foreseen for models should in any case not apply when an own 
model is used for purely internal processes that are not essential for providing a product or 
a service to third parties and the rights of natural persons are not affected. Considering 
their potential significantly negative effects, the general-purpose AI models with systemic 
risk should always be subject to the relevant obligations under this Regulation. The 
definition should not cover AI models used before their placing on the market for the sole 
purpose of research, development and prototyping activities. This is without prejudice to 
the obligation to comply with this Regulation when, following such activities, a model is 
placed on the market.

Whereas the generality of a model could, among other criteria, also be determined by a 
number of parameters, models with at least a billion of parameters and trained with a large 
amount of data using self-supervision at scale should be considered as displaying 
significant generality and competently performing a wide range of distinctive tasks.

Large generative AI models are a typical example for a general-purpose AI model, given 
that they allow for flexible generation of content (such as in the form of text, audio, images 
or video) that can readily accommodate a wide range of distinctive tasks.

When a general-purpose AI model is integrated into or forms part of an AI system, this 
system should be considered a general-purpose AI system when, due to this integration, 
this system has the capability to serve a variety of purposes. A general-purpose AI system 
can be used directly, or it may be integrated into other AI systems.

Providers of general purpose AI models have a particular role and responsibility in the AI 
value chain, as the models they provide may form the basis for a range of downstream
systems, often provided by downstream providers that necessitate a good understanding of 
the models and their capabilities, both to enable the integration of such models into their 
products, and to fulfil their obligations under this or other regulations. Therefore, 
proportionate transparency measures should be foreseen, including the drawing up and 
keeping up to date of documentation, and the provision of information on the general 
purpose AI model for its usage by the downstream providers. Technical documentation 
should be prepared and kept up to date by the general purpose AI model provider for the 
purpose of making it available, upon request, to the AI Office and the national competent 
authorities. The minimal set of elements contained in such documentations should be 
outlined, respectively, in Annex (IXb) and Annex (IXa). The Commission should be 
enabled to amend the Annexes by delegated acts in the light of the evolving technological 
developments.

General purpose models, in particular large generative models, capable of generating text, 
images, and other content, present unique innovation opportunities but also challenges to 
artists, authors, and other creators and the way their creative content is created, distributed, 
used and consumed. The development and training of such models require access to vast 
amounts of text, images, videos, and other data. Text and data mining techniques may be 
used extensively in this context for the retrieval and analysis of such content, which may 
be protected by copyright and related rights. Any use of copyright protected content 
requires the authorization of the rightholder concerned unless relevant copyright 
exceptions and limitations apply. Directive (EU) 2019/790 introduced exceptions and 
limitations allowing reproductions and extractions of works or other subject matter, for the 
purposes of text and data mining, under certain conditions. Under these rules, rightholders 
may choose to reserve their rights over their works or other subject matter to prevent text 
and data mining, unless this is done for the purposes of scientific research. Where the 
rights to opt out has been expressly reserved in an appropriate manner, providers of 
general-purpose AI models need to obtain an authorisation from rightholders if they want 
to carry out text and data mining over such works.

 Providers that place general purpose AI models on the EU market should ensure 
compliance with the relevant obligations in this Regulation. For this purpose, providers of 
general purpose AI models should put in place a policy to respect Union law on copyright 
and related rights, in particular to identify and respect the reservations of rights expressed 
by rightholders pursuant to Article 4(3) of Directive (EU) 2019/790. Any provider placing 
a general purpose AI model on the EU market should comply with this obligation, 
regardless of the jurisdiction in which the copyright-relevant acts underpinning the training 
of these general purpose AI models take place. This is necessary to ensure a level playing 
field among providers of general purpose AI models where no provider should be able to 
gain a competitive advantage in the EU market by applying lower copyright standards than 
those provided in the Union. 

 General purpose AI models could pose systemic risks which include, but are not limited to, 
any actual or reasonably foreseeable negative effects in relation to major accidents, 
disruptions of critical sectors and serious consequences to public health and safety; any 
actual or reasonably foreseeable negative effects on democratic processes, public and 
economic security; the dissemination of illegal, false, or discriminatory content. Systemic 
risks should be understood to increase with model capabilities and model reach, can arise 
along the entire lifecycle of the model, and are influenced by conditions of misuse, model 
reliability, model fairness and model security, the degree of autonomy of the model, its 
access to tools, novel or combined modalities, release and distribution strategies, the 
potential to remove guardrails and other factors. In particular, international approaches 
have so far identified the need to devote attention to risks from potential intentional misuse 
or unintended issues of control relating to alignment with human intent; chemical, 
biological, radiological, and nuclear risks, such as the ways in which barriers to entry can 
be lowered, including for weapons development, design acquisition, or use; offensive 
cyber capabilities, such as the ways in vulnerability discovery, exploitation, or operational 
use can be enabled; the effects of interaction and tool use, including for example the 
capacity to control physical systems and interfere with critical infrastructure; risks from 
models of making copies of themselves or “self-replicating” or training other models; the 
ways in which models can give rise to harmful bias and discrimination with risks to 
individuals, communities or societies; the facilitation of disinformation or harming privacy 
with threats to democratic values and human rights; risk that a particular event could lead 
to a chain reaction with considerable negative effects that could affect up to an entire city, 
an entire domain activity or an entire community.

 It is appropriate to establish a methodology for the classification of general purpose AI 
models as general purpose AI model with systemic risks. Since systemic risks result from 
particularly high capabilities, a general-purpose AI models should be considered to present 
systemic risks if it has high-impact capabilities, evaluated on the basis of appropriate 
technical tools and methodologies, or significant impact on the internal market due to its 
reach. High-impact capabilities in general purpose AI models means capabilities that 
match or exceed the capabilities recorded in the most advanced general-purpose AI 
models. The full range of capabilities in a model could be better understood after its release 
on the market or when users interact with the model. According to the state of the art at the 
time of entry into force of this Regulation, the cumulative amount of compute used for the 
training of the general purpose AI model measured in floating point operations (FLOPs) is 
one of the relevant approximations for model capabilities. The amount of compute used for 
training cumulates the compute used across the activities and methods that are intended to 
enhance the capabilities of the model prior to deployment, such as pre-training, synthetic 
data generation and fine-tuning. Therefore, an initial threshold of FLOPs should be set, 
which, if met by a general-purpose AI model, leads to a presumption that the model is a 
general-purpose AI model with systemic risks. This threshold should be adjusted over time 
to reflect technological and industrial changes, such as algorithmic improvements or 
increased hardware efficiency, and should be supplemented with benchmarks and 
indicators for model capability. To inform this, the AI Office should engage with the 
scientific community, industry, civil society and other experts. Thresholds, as well as tools 
and benchmarks for the assessment of high-impact capabilities, should be strong predictors 
of generality, its capabilities and associated systemic risk of general-purpose AI models, 
and could take into taking into account the way the model will be placed on the market or 
the number of users it may affect. To complement this system, there should be a possibility 
for the Commission to take individual decisions designating a general-purpose AI model as 
a general-purpose AI model with systemic risk if it is found that such model has 
capabilities or impact equivalent to those captured by the set threshold. This decision 
should be taken on the basis of an overall assessment of the criteria set out in Annex YY, 
such as quality or size of the training data set, number of business and end users, its input 
and output modalities, its degree of autonomy and scalability, or the tools it has access to. 
Upon a reasoned request of a provider whose model has been designated as a general-
purpose AI model with systemic risk, the Commission should take the request into account 
and may decide to reassess whether the general-purpose AI model can still be considered 
to present systemic risks

The providers of general-purpose AI models presenting systemic risks should be subject, in 
addition to the obligations provided for providers of general purpose AI models, to 
obligations aimed at identifying and mitigating those risks and ensuring an adequate level 
of cybersecurity protection, regardless of whether it is provided as a standalone model or 
embedded in an AI system or a product. To achieve these objectives, the Regulation should 
require providers to perform the necessary model evaluations, in particular prior to its first 
placing on the market, including conducting and documenting adversarial testing of 
models, also, as appropriate, through internal or independent external testing. In addition, 
providers of general-purpose AI models with systemic risks should continuously assess 
and mitigate systemic risks, including for example by putting in place risk management 
policies, such as accountability and governance processes, implementing post-market 
monitoring, taking appropriate measures along the entire model’s lifecycle and cooperating 
with relevant actors across the AI value chain.

This Regulation regulates AI systems and models by imposing certain requirements and 
obligations for relevant market actors that are placing them on the market, putting into 
service or use in the Union, thereby complementing obligations for providers of 
intermediary services that embed such systems or models into their services regulated by 
Regulation (EU) 2022/2065. To the extent that such systems or models are embedded into 
designated very large online platforms or very large online search engines, they are subject 
to the risk management framework provided for in Regulation (EU) 2022/2065. 
Consequently, the corresponding obligations of the AI Act should be presumed to be 
fulfilled, unless significant systemic risks not covered by Regulation (EU) 2022/2065 
emerge and are identified in such models. Within this framework, providers of very large 
online platforms and very large search engines are obliged to assess potential systemic 
risks stemming from the design, functioning and use of their services, including how the 
design of algorithmic systems used in the service may contribute to such risks, as well as 
systemic risks stemming from potential misuses. Those providers are also obliged to take 
appropriate mitigating measures in observance of fundamental rights.

 Furthermore, obligations placed on providers and deployers of certain AI systems in this 
Regulation to enable the detection and disclosure that the outputs of those systems are 
artificially generated or manipulated are particularly relevant to facilitate the effective 
implementation of Regulation (EU) 2022/2065. This applies in particular as regards the 
obligations of providers of very large online platforms or very large online search engines 
to identify and mitigate systemic risks that may arise from the dissemination of content that 
has been artificially generated or manipulated, in particular risk of the actual or foreseeable 
negative effects on democratic processes, civic discourse and electoral processes, including 
through disinformation.

 In line with the commonly established notion of substantial modification for products 
regulated by Union harmonisation legislation, it is appropriate that whenever a change 
occurs which may affect the compliance of a high risk AI system with this Regulation (e.g. 
change of operating system or software architecture), or when the intended purpose of the 
system changes, that AI system should be considered a new AI system which should 
undergo a new conformity assessment. However, changes occurring to the algorithm and 
the performance of AI systems which continue to ‘learn’ after being placed on the market 
or put into service (i.e. automatically adapting how functions are carried out) should not 
constitute a substantial modification, provided that those changes have been pre-
determined by the provider and assessed at the moment of the conformity assessment.

In order to facilitate the work of the Commission and the Member States in the artificial 
intelligence field as well as to increase the transparency towards the public, providers of 
high-risk AI systems other than those related to products falling within the scope of 
relevant existing Union harmonisation legislation, as well as providers who consider that 
an AI system referred to in annex III is by derogation not high-risk, should be required to 
register themselves and information about their AI system in a EU database, to be 
established and managed by the Commission. Before using a high-risk AI system listed in 
Annex III, deployers of high-risk AI systems that are public authorities, agencies or bodies, 
shall register themselves in such database and select the system that they envisage to use.. 
Other deployers should be entitled to do so voluntarily. This section of the database should 
be publicly accessible, free of charge, the information should be easily navigable, 
understandable and machine-readable. The database should also be user-friendly, for 
example by providing search functionalities, including through keywords, allowing the 
general public to find relevant information included in Annex VIII and on the areas of risk 
under Annex III to which the high-risk AI systems correspond. Any substantial 
modification of high-risk AI systems should also be registered in the EU database. For 
high risk AI systems in the area of law enforcement, migration, asylum and border control 
management, the registration obligations should be fulfilled in a secure non-public section 
of the database. Access to the secure non-public section should be strictly limited to the 
Commission as well as to market surveillance authorities with regard to their national 
section of that database. High risk AI systems in the area of critical infrastructure should 
only be registered at national level. The Commission should be the controller of the EU 
database, in accordance with Regulation (EU) 2018/1725 of the European Parliament and 
of the Council26. In order to ensure the full functionality of the database, when deployed, 
the procedure for setting the database should include the elaboration of functional 
specifications by the Commission and an independent audit report. The Commission 
should take into account cybersecurity and hazard-related risks when carrying out its tasks 
as data controller on the EU database. In order to maximise the availability and use of the 
database by the public, the database, including the information made available through it, 
should comply with requirements under the Directive 2019/882.

 A variety of AI systems can generate large quantities of synthetic content that becomes 
increasingly hard for humans to distinguish from human-generated and authentic content. 
The wide availability and increasing capabilities of those systems have a significant impact 
on the integrity and trust in the information ecosystem, raising new risks of misinformation 
and manipulation at scale, fraud, impersonation and consumer deception. In the light of 
those impacts, the fast technological pace and the need for new methods and techniques to 
trace origin of information, it is appropriate to require providers of those systems to embed 
technical solutions that enable marking in a machine readable format and detection that the 
output has been generated or manipulated by an AI system and not a human. Such 
techniques and methods should be sufficiently reliable, interoperable, effective and robust 
as far as this is technically feasible, taking into account available techniques or a 
combination of such techniques, such as watermarks, metadata identifications, 
cryptographic methods for proving provenance and authenticity of content, logging 
methods, fingerprints or other techniques, as may be appropriate. When implementing this 
obligation, providers should also take into account the specificities and the limitations of 
the different types of content and the relevant technological and market developments in 
the field, as reflected in the generally acknowledged state-of-the-art. Such techniques and 
methods can be implemented at the level of the system or at the level of the model, 
including general-purpose AI models generating content, thereby facilitating fulfilment of 
this obligation by the downstream provider of the AI system. To remain proportionate, it is
appropriate to envisage that this marking obligation should not cover AI systems 
performing primarily an assistive function for standard editing or AI systems not 
substantially altering the input data provided by the deployer or the semantics thereof. 

 Further to the technical solutions employed by the providers of the system, deployers , who 
use an AI system to generate or manipulate image, audio or video content that appreciably 
resembles existing persons, places or events and would falsely appear to a person to be 
authentic (‘deep fakes’), should also clearly and distinguishably disclose that the content 
has been artificially created or manipulated by labelling the artificial intelligence output 
accordingly and disclosing its artificial origin The compliance with this transparency 
obligation should not be interpreted as indicating that the use of the system or its output 
impedes the right to freedom of expression and the right to freedom of the arts and sciences 
guaranteed in the Charter of Fundamental Rights of the EU, in particular where the content 
is part of an evidently creative, satirical, artistic or fictional work or programme, subject to 
appropriate safeguards for the rights and freedoms of third parties. In those cases, the 
transparency obligation for deep fakes set out in this Regulation is limited to disclosure of 
the existence of such generated or manipulated content in an appropriate manner that does 
not hamper the display or enjoyment of the work, including its normal exploitation and 
use, while maintaining the utility and quality of the work. In addition, it is also appropriate 
to envisage a similar disclosure obligation in relation to AI-generated or manipulated text 
to the extent it is published with the purpose of informing the public on matters of public 
interest unless the AI-generated content has undergone a process of human review or 
editorial control and a natural or legal person holds editorial responsibility for the 
publication of the content. 

 In order to accelerate the process of development and placing on the market of high-risk AI 
systems listed in Annex III, it is important that providers or prospective providers of such 
systems may also benefit from a specific regime for testing those systems in real world 
conditions, without participating in an AI regulatory sandbox. However, in such cases and 
taking into account the possible consequences of such testing on individuals, it should be 
ensured that appropriate and sufficient guarantees and conditions are introduced by the 
Regulation for providers or prospective providers. Such guarantees should include, among 
others, requesting informed consent of natural persons to participate in testing in real world 
conditions, with the exception of law enforcement in cases where the seeking of informed 
consent would prevent the AI system from being tested. Consent of subjects to participate 
in such testing under this Regulation is distinct from and without prejudice to consent of 
data subjects for the processing of their personal data under the relevant data protection 
law. It is also important to minimise the risks and enable oversight by competent 
authorities and therefore require prospective providers to have a real-world testing plan 
submitted to competent market surveillance authority, register the testing in dedicated 
sections in the EU-wide database subject to some limited exceptions, set limitations on the 
period for which the testing can be done and require additional safeguards for persons 
belonging to certain vulnerable groups as well as a written agreement defining the roles 
and responsibilities of prospective providers and deployers and effective oversight by 
competent personnel involved in the real world testing. Furthermore, it is appropriate to 
envisage additional safeguards to ensure that the predictions, recommendations or 
decisions of the AI system can be effectively reversed and disregarded and that personal 
data is protected and is deleted when the subjects have withdrawn their consent to 
participate in the testing without prejudice to their rights as data subjects under the EU data 
protection law. As regards transfer of data, it is also appropriate to envisage that data 
collected and processed for the purpose of the testing in real world conditions should only 
be transferred to third countries outside the Union provided appropriate and applicable 
safeguards under Union law are implemented, notably in accordance with bases for 
transfer of personal data under Union law on data protection, while for non-personal data 
appropriate safeguards are put in place in accordance with Union law, such as the Data 
Governance Act and the Data Act.

In order to ensure that providers of high-risk AI systems can take into account the 
experience on the use of high-risk AI systems for improving their systems and the design 
and development process or can take any possible corrective action in a timely manner, all 
providers should have a post-market monitoring system in place. Where relevant, post-
market monitoring should include an analysis of the interaction with other AI systems
including other devices and software. Post-market monitoring should not cover sensitive 
operational data of deployers which are law enforcement authorities. This system is also 
key to ensure that the possible risks emerging from AI systems which continue to ‘learn’ 
after being placed on the market or put into service can be more efficiently and timely 
addressed. In this context, providers should also be required to have a system in place to 
report to the relevant authorities any serious incidents resulting from the use of their AI 
systems, meaning incident or malfunctioning leading to death or serious damage to health, 
serious and irreversible disruption of the management and operation of critical 
infrastructure, breaches of obligations under Union law intended to protect fundamental 
rights or serious damage to property or the environment. 

It is important that AI systems related to products that are not high-risk in accordance with 
this Regulation and thus are not required to comply with the requirements set out for high-
risk AI systems are nevertheless safe when placed on the market or put into service. To 
contribute to this objective, Regulation (EU) 2023/988 of the European Parliament and of 
the Council28 would apply as a safety net.

